{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/germa/anaconda3/envs/project/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/var/folders/c6/k78nzngn7nbdd0786bxl3s0c0000gn/T/ipykernel_1857/4227447418.py:2: DeprecationWarning: \n",
      "Pyarrow will become a required dependency of pandas in the next major release of pandas (pandas 3.0),\n",
      "(to allow more performant data types, such as the Arrow string type, and better interoperability with other libraries)\n",
      "but was not found to be installed on your system.\n",
      "If this would cause problems for you,\n",
      "please provide us feedback at https://github.com/pandas-dev/pandas/issues/54466\n",
      "        \n",
      "  import pandas as pd\n"
     ]
    }
   ],
   "source": [
    "from transformers import XLMRobertaTokenizer, XLMRobertaModel\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "from torch import Tensor\n",
    "from typing import List, Tuple\n",
    "from torch.utils.data import TensorDataset, DataLoader, random_split\n",
    "import sys\n",
    "sys.path.append('/Users/germa/thesis/bachelor_project2023/src/models/lstm')\n",
    "from xlm_roberta import XLMRoberta\n",
    "sys.path.append('/Users/germa/thesis/bachelor_project2023/src/utils')\n",
    "from chunker import Chunker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMOnXLMRoberta(nn.Module):\n",
    "    def __init__(self, lstm_hidden_size, num_lstm_layers, model_name = 'xlm-roberta-base', train_path = '../../../data/train/train.csv', train_size = 0.8, batch_size = 32, shuffle = True):\n",
    "        super(LSTMOnXLMRoberta, self).__init__()\n",
    "        self.xlmroberta_model = XLMRoberta(model_name)\n",
    "        self.tokenizer = XLMRobertaTokenizer.from_pretrained(model_name)\n",
    "        self.chunker = Chunker(self.tokenizer, 512)\n",
    "        self.lstm = nn.LSTM(input_size=768,\n",
    "                            hidden_size=lstm_hidden_size,\n",
    "                            num_layers=num_lstm_layers,\n",
    "                            batch_first=True,\n",
    "                            bidirectional=False)\n",
    "        self.fc = nn.Linear(lstm_hidden_size * 2, 1)\n",
    "\n",
    "        self.train_df, self.train_label = self.get_data(train_path)\n",
    "    \n",
    "    def get_data(self, path):\n",
    "        train_df = pd.read_csv(path)\n",
    "        train_short_df = train_df.head(2)\n",
    "        return train_short_df, train_df[\"overall\"]\n",
    "\n",
    "    def chunk_data(self, df):\n",
    "        texts1, texts2 = df[\"text1\"], df[\"text2\"]\n",
    "        input_ids = []\n",
    "        for i in range(len(texts1)):\n",
    "            input_id_1 = self.chunker.chunk(texts1[i])\n",
    "            input_id_2 = self.chunker.chunk(texts2[i])\n",
    "            input_ids.append([input_id_1, input_id_2])           \n",
    "        return input_ids\n",
    "        \n",
    "    def _manage_device(self) -> None:\n",
    "        \"\"\"\n",
    "            Manage the device to run the model on\n",
    "        \"\"\"\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.xlmroberta_model.to(self.device)\n",
    "    \n",
    "    def get_embeddings(self, input_ids):\n",
    "        \"\"\"\n",
    "            Get the embeddings from the XLM-Roberta model\n",
    "        \"\"\"\n",
    "        #input_ids = input_ids.to(self.device)\n",
    "        outputs = self.xlmroberta_model.run(input_ids)\n",
    "        return outputs\n",
    "    \n",
    "    def get_loss(self, outputs, labels):\n",
    "        # Instantiate nn.MSELoss\n",
    "        loss_function = nn.MSELoss()\n",
    "        # Compute the loss\n",
    "        return loss_function(outputs, labels)\n",
    "    \n",
    "     \n",
    "    def train(self):\n",
    "        self._manage_device()\n",
    "        train_data = self.chunk_data(self.train_df)\n",
    "        labels = self.train_label\n",
    "        input_embeddings =self.get_embeddings(train_data)\n",
    "        for i, row in enumerate(input_embeddings):\n",
    "            lstm_output_row = []\n",
    "            for text in row:\n",
    "                lstm_out, _ = self.lstm(text)\n",
    "                lstm_out_last = lstm_out[:, -1, :]  # Extract output of the last time step\n",
    "                lstm_output_row.append(lstm_out_last)\n",
    "            nn_input = torch.cat(lstm_output_row, dim=1)\n",
    "            output = self.fc(nn_input)\n",
    "            label = torch.tensor(labels[i], dtype=torch.float32)\n",
    "            loss = self.get_loss(output, label)\n",
    "            loss.backward()\n",
    "            \n",
    "                \n",
    "\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (748 > 512). Running this sequence through the model will result in indexing errors\n",
      "/Users/germa/anaconda3/envs/project/lib/python3.12/site-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([])) that is different to the input size (torch.Size([1, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    }
   ],
   "source": [
    "lstmonxlmroberta = LSTMOnXLMRoberta(2, 1)\n",
    "lstmonxlmroberta.train()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
