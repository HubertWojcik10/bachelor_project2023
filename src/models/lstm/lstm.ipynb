{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import XLMRobertaTokenizer, XLMRobertaModel\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "from torch import Tensor\n",
    "from typing import List, Tuple\n",
    "from torch.utils.data import TensorDataset, DataLoader, random_split\n",
    "import sys\n",
    "sys.path.append('/Users/germa/thesis/bachelor_project2023/src/models/lstm')\n",
    "from xlm_roberta import XLMRoberta\n",
    "sys.path.append('/Users/germa/thesis/bachelor_project2023/src/utils')\n",
    "from chunker import Chunker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMOnXLMRoberta(nn.Module):\n",
    "    def __init__(self, input_size, lstm_hidden_size, num_lstm_layers, model_name = 'xlm-roberta-base', train_path = '../../../data/train/train.csv', train_size = 0.8, batch_size = 32, shuffle = True):\n",
    "        super(LSTMOnXLMRoberta, self).__init__()\n",
    "        self.xlmroberta_model = XLMRoberta(model_name)\n",
    "        self.tokenizer = XLMRobertaTokenizer.from_pretrained(model_name)\n",
    "        self.chunker = Chunker(self.tokenizer, 512)\n",
    "        self.lstm = nn.LSTM(input_size=input_size,\n",
    "                            hidden_size=lstm_hidden_size,\n",
    "                            num_layers=num_lstm_layers,\n",
    "                            batch_first=True,\n",
    "                            bidirectional=False)\n",
    "        self.fc = nn.Linear(lstm_hidden_size * 2, 1)\n",
    "\n",
    "        self.optimizer = torch.optim.AdamW(self.fc.parameters(), lr=1e-5)\n",
    "        self.loss_function = nn.MSELoss()\n",
    "\n",
    "        self.train_df, self.train_label = self.get_data(train_path)\n",
    "    \n",
    "    def get_data(self, path):\n",
    "        train_df = pd.read_csv(path)\n",
    "        train_short_df = train_df.head(2)\n",
    "        return train_short_df, train_df[\"overall\"]\n",
    "\n",
    "    def chunk_data(self, df):\n",
    "        texts1, texts2 = df[\"text1\"], df[\"text2\"]\n",
    "        input_ids = []\n",
    "        for i in range(len(texts1)):\n",
    "            input_id_1 = self.chunker.chunk(texts1[i])\n",
    "            input_id_2 = self.chunker.chunk(texts2[i])\n",
    "            input_ids.append([input_id_1, input_id_2])           \n",
    "        return input_ids\n",
    "    \n",
    "    def batch_data(self, input_ids, labels):\n",
    "        #convert to batches of 50% of the data without tensor\n",
    "        input_ids = torch.tensor(input_ids)\n",
    "        labels = torch.tensor(labels)\n",
    "        dataset = TensorDataset(input_ids, labels)\n",
    "        train_size = int(0.8 * len(dataset))\n",
    "        val_size = len(dataset) - train_size\n",
    "        train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
    "        train_dataloader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "        val_dataloader = DataLoader(val_dataset, batch_size=32, shuffle=True)\n",
    "        return train_dataloader, val_dataloader\n",
    "        \n",
    "    def _manage_device(self) -> None:\n",
    "        \"\"\"\n",
    "            Manage the device to run the model on\n",
    "        \"\"\"\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.xlmroberta_model.to(self.device)\n",
    "        self.lstm.to(self.device)\n",
    "    \n",
    "    def get_embeddings(self, input_ids):\n",
    "        \"\"\"\n",
    "            Get the embeddings from the XLM-Roberta model\n",
    "        \"\"\"\n",
    "        #input_ids = input_ids.to(self.device)\n",
    "        outputs = self.xlmroberta_model.run(input_ids)\n",
    "        return outputs\n",
    "    \n",
    "    def test_batch(self):\n",
    "        train_dataloader, val_dataloader = self.batch_data(self.train_df, self.train_label)\n",
    "        for batch in train_dataloader:\n",
    "            print(batch)\n",
    "            print(\"Done\")\n",
    "     \n",
    "    def train(self):\n",
    "        self._manage_device()\n",
    "        train_data = self.chunk_data(self.train_df)\n",
    "        labels = self.train_label\n",
    "        input_embeddings = self.get_embeddings(train_data)\n",
    "        batch_loss = 0\n",
    "        for i, row in enumerate(input_embeddings):\n",
    "            print(row.shape)\n",
    "            lstm_out, _ = self.lstm(row)\n",
    "            lstm_out_last = lstm_out[:, -1, :]  # Extract output of the last time step\n",
    "            nn = lstm_out_last.reshape(1, -1) #concatenate the two outputs\n",
    "            output = self.fc(nn)\n",
    "            print(output)\n",
    "            label = torch.tensor(labels[i], dtype=torch.float32)\n",
    "            print(label)\n",
    "            loss = self.loss_function(output, label)\n",
    "            batch_loss += loss\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        print(batch_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (748 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 1, 768])\n",
      "tensor([[-0.0748]], grad_fn=<AddmmBackward0>)\n",
      "tensor(4.)\n",
      "torch.Size([2, 2, 768])\n",
      "tensor([[-0.0967]], grad_fn=<AddmmBackward0>)\n",
      "tensor(3.6667)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/germa/anaconda3/envs/project/lib/python3.12/site-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([])) that is different to the input size (torch.Size([1, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(30.7673, grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "lstmonxlmroberta = LSTMOnXLMRoberta(768, 384, 1)\n",
    "lstmonxlmroberta.train()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
