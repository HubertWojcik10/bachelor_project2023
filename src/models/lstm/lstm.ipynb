{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/germa/anaconda3/envs/project/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/var/folders/c6/k78nzngn7nbdd0786bxl3s0c0000gn/T/ipykernel_2902/4227447418.py:2: DeprecationWarning: \n",
      "Pyarrow will become a required dependency of pandas in the next major release of pandas (pandas 3.0),\n",
      "(to allow more performant data types, such as the Arrow string type, and better interoperability with other libraries)\n",
      "but was not found to be installed on your system.\n",
      "If this would cause problems for you,\n",
      "please provide us feedback at https://github.com/pandas-dev/pandas/issues/54466\n",
      "        \n",
      "  import pandas as pd\n"
     ]
    }
   ],
   "source": [
    "from transformers import XLMRobertaTokenizer, XLMRobertaModel\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "from torch import Tensor\n",
    "from typing import List, Tuple\n",
    "from torch.utils.data import TensorDataset, DataLoader, random_split\n",
    "import sys\n",
    "sys.path.append('/Users/germa/thesis/bachelor_project2023/src/models/lstm')\n",
    "from xlm_roberta import XLMRoberta\n",
    "sys.path.append('/Users/germa/thesis/bachelor_project2023/src/utils')\n",
    "from chunker import Chunker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMOnXLMRoberta(nn.Module):\n",
    "    def __init__(self, input_size, lstm_hidden_size, num_lstm_layers, model_name = 'xlm-roberta-base', train_path = '../../../data/train/train.csv', train_size = 0.8, batch_size = 32, shuffle = True):\n",
    "        super(LSTMOnXLMRoberta, self).__init__()\n",
    "        self.xlmroberta_model = XLMRoberta(model_name)\n",
    "        self.tokenizer = XLMRobertaTokenizer.from_pretrained(model_name)\n",
    "        self.chunker = Chunker(self.tokenizer, 512)\n",
    "        self.lstm = nn.LSTM(input_size=input_size,\n",
    "                            hidden_size=lstm_hidden_size,\n",
    "                            num_layers=num_lstm_layers,\n",
    "                            batch_first=True,\n",
    "                            bidirectional=False)\n",
    "        self.fc = nn.Linear(lstm_hidden_size * 2, 1)\n",
    "\n",
    "        parameters_to_optimize = list(self.fc.parameters()) + list(self.lstm.parameters()) + list(self.xlmroberta_model.parameters())\n",
    "\n",
    "        self.optimizer = torch.optim.AdamW(parameters_to_optimize, lr=1e-5)\n",
    "        self.loss_function = nn.MSELoss()\n",
    "        self.train_df= self.get_data(train_path)\n",
    "        torch.manual_seed(42)\n",
    "\n",
    "    def parameter_to_optimize(self):\n",
    "        for param in self.xlmroberta_model.parameters():\n",
    "            param.requires_grad = True\n",
    "        for param in self.lstm.parameters():\n",
    "            param.requires_grad = True\n",
    "        for param in self.fc.parameters():\n",
    "            param.requires_grad = True\n",
    "\n",
    "    def get_data(self, path):\n",
    "        train_df = pd.read_csv(path)\n",
    "        train_short_df = train_df.head(500)\n",
    "        return train_short_df\n",
    "    \n",
    "    def chunk_data(self, df):\n",
    "        texts1, texts2, labels = df[\"text1\"], df[\"text2\"], df[\"overall\"]\n",
    "        input_ids = []\n",
    "        for i in range(len(texts1)):\n",
    "            input_id_1 = self.chunker.chunk(texts1[i])\n",
    "            input_id_2 = self.chunker.chunk(texts2[i])\n",
    "            input_ids.append([input_id_1, input_id_2])           \n",
    "        return input_ids, labels\n",
    "    \n",
    "        \n",
    "    def _manage_device(self) -> None:\n",
    "        \"\"\"\n",
    "            Manage the device to run the model on\n",
    "        \"\"\"\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.xlmroberta_model.to(self.device)\n",
    "        self.lstm.to(self.device)\n",
    "        self.fc.to(self.device)\n",
    "    \n",
    "    def get_embeddings(self, input_ids):\n",
    "        \"\"\"\n",
    "            Get the embeddings from the XLM-Roberta model\n",
    "        \"\"\"\n",
    "        outputs = self.xlmroberta_model.run(input_ids)\n",
    "        return outputs\n",
    "\n",
    "\n",
    "    def pad_to_same_size(self, tensors):\n",
    "        max_size = max(tensor.size(0) for tensor in tensors)\n",
    "        padded_tensors = []\n",
    "        for tensor in tensors:\n",
    "            if tensor.size(0) < max_size:\n",
    "                padding = torch.zeros(max_size - tensor.size(0), tensor.size(1))\n",
    "                padded_tensor = torch.cat((tensor, padding), dim=0)\n",
    "                padded_tensors.append(padded_tensor)\n",
    "            else:\n",
    "                padded_tensors.append(tensor)\n",
    "        return torch.stack(padded_tensors)\n",
    "    \n",
    "    def pearson_correlation(self, labels_val, eval_loss):\n",
    "        print(f\"Labels: {labels_val}\")\n",
    "        print(f\"Eval loss: {eval_loss}\")\n",
    "\n",
    "        eval_loss= np.array([t.item() for t in eval_loss])\n",
    "        return np.corrcoef(labels_val, eval_loss)[0][1]\n",
    "\n",
    "    \n",
    "    def evaluate_model(self, input_ids_val, labels_val, batch_size = 4):\n",
    "        \"\"\"\n",
    "            Evaluate the model \"\"\" \n",
    "        self.eval()\n",
    "        output_val = []\n",
    "        with torch.no_grad():\n",
    "            for i in range(0, len(input_ids_val), batch_size):\n",
    "                input_batch_data = input_ids_val[i:i + batch_size]\n",
    "                input_batch = self.get_embeddings(input_batch_data)\n",
    "                label_batch = labels_val[i:i + batch_size]\n",
    "\n",
    "                for row, label in zip(input_batch, label_batch):\n",
    "                    index1, index2 = len(row[0]), len(row[1])\n",
    "                    row_padded = self.pad_to_same_size(row) \n",
    "                    lstm_out, _ = self.lstm(row_padded)\n",
    "                    lstm_out_last1 = lstm_out[0, index1 - 1, :]\n",
    "                    lstm_out_last2 = lstm_out[1, index2 - 1, :]\n",
    "                    nn = torch.cat((lstm_out_last1, lstm_out_last2), 0)\n",
    "                    output = self.fc(nn)\n",
    "                    output_val.append(output)\n",
    "                    \n",
    "        pearson = self.pearson_correlation(labels_val, output_val)\n",
    "        return pearson\n",
    "    \n",
    "    def train_model(self, input_ids_train, labels_train, input_ids_val, labels_val, batch_size = 4):\n",
    "        \"\"\"\n",
    "            Train the model\n",
    "        \"\"\"\n",
    "        best_pearson = -1\n",
    "        self.train()\n",
    "        for epoch in range(5):\n",
    "            print(f\"Epoch: {epoch}\")\n",
    "\n",
    "            for i in range(0, len(input_ids_train), batch_size):\n",
    "                print(f\"Batch: {i/batch_size}\")\n",
    "                input_batch_data = input_ids_train[i:i + batch_size]\n",
    "                input_batch = self.get_embeddings(input_batch_data)\n",
    "                label_batch = labels_train[i:i + batch_size]\n",
    "                batch_loss = 0\n",
    "                self.optimizer.zero_grad()  # Clear gradients\n",
    "                \n",
    "                for row, label in zip(input_batch, label_batch):\n",
    "                    index1, index2 = len(row[0]), len(row[1])\n",
    "                    row_padded = self.pad_to_same_size(row) \n",
    "                    lstm_out, _ = self.lstm(row_padded)\n",
    "                    lstm_out_last1 = lstm_out[0, index1 - 1, :]\n",
    "                    lstm_out_last2 = lstm_out[1, index2 - 1, :]\n",
    "                    nn = torch.cat((lstm_out_last1, lstm_out_last2), 0)\n",
    "                    output = self.fc(nn)\n",
    "                    loss = self.loss_function(output, torch.tensor(label, dtype=torch.float32).view(1))\n",
    "                    batch_loss = batch_loss + loss\n",
    "    \n",
    "                batch_loss = batch_loss/batch_size \n",
    "                batch_loss.backward()  # Backpropagation \n",
    "                torch.nn.utils.clip_grad_norm_(self.parameters(), 1.0)\n",
    "                self.optimizer.step()  # Update weights\n",
    "                print(f\"Batch loss: {batch_loss}\")\n",
    "        \n",
    "            eval_pearson = self.evaluate_model(input_ids_val, labels_val)\n",
    "            print(f\"Eval pearson: {eval_pearson}\")\n",
    "            if eval_pearson > best_pearson:\n",
    "                best_pearson = eval_pearson\n",
    "                print(\"Saving the model...\")\n",
    "                torch.save(self.state_dict(), \"../../../saved_models/xlmroberta_on_lstm/best_lstm.pth\")\n",
    "\n",
    "    def run(self):\n",
    "        torch.autograd.set_detect_anomaly(True)\n",
    "        self._manage_device()\n",
    "        #shuffle data\n",
    "        train_df = self.train_df.sample(frac=1).reset_index(drop=True)\n",
    "        train_df, val_df = train_df[:int(len(train_df) * 0.8)], train_df[int(len(train_df) * 0.8):].reset_index(drop=True)\n",
    "        input_ids_train, labels_train = self.chunk_data(train_df)\n",
    "        input_ids_val, labels_val = self.chunk_data(val_df)\n",
    "        self.parameter_to_optimize()\n",
    "        self.train_model(input_ids_train, labels_train, input_ids_val, labels_val)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (515 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Labels: 0     1.000000\n",
      "1     4.000000\n",
      "2     1.666667\n",
      "3     3.750000\n",
      "4     1.000000\n",
      "        ...   \n",
      "95    4.000000\n",
      "96    4.000000\n",
      "97    2.333333\n",
      "98    4.000000\n",
      "99    1.000000\n",
      "Name: overall, Length: 100, dtype: float64\n",
      "Eval loss: [tensor([-0.0347]), tensor([0.0046]), tensor([0.0097]), tensor([-0.0053]), tensor([0.0024]), tensor([0.0107]), tensor([0.0226]), tensor([0.0165]), tensor([0.0037]), tensor([0.0112]), tensor([0.0056]), tensor([0.0088]), tensor([-0.0055]), tensor([0.0079]), tensor([0.0032]), tensor([0.0332]), tensor([-0.0227]), tensor([0.0018]), tensor([0.0388]), tensor([0.0351]), tensor([0.0081]), tensor([0.0369]), tensor([-0.0232]), tensor([0.0048]), tensor([-0.0098]), tensor([0.0156]), tensor([0.0339]), tensor([0.0267]), tensor([-0.0146]), tensor([-0.0113]), tensor([-0.0306]), tensor([0.0383]), tensor([-0.0298]), tensor([0.0396]), tensor([0.0322]), tensor([-0.0232]), tensor([0.0258]), tensor([-0.0488]), tensor([-0.0240]), tensor([0.0126]), tensor([-0.0094]), tensor([0.0204]), tensor([0.0022]), tensor([0.0422]), tensor([-0.0231]), tensor([-0.0315]), tensor([0.0052]), tensor([0.0138]), tensor([-0.0308]), tensor([0.0232]), tensor([-0.0065]), tensor([0.0021]), tensor([-0.0359]), tensor([-0.0188]), tensor([-0.0193]), tensor([0.0144]), tensor([-0.0099]), tensor([-0.0097]), tensor([-0.0024]), tensor([0.0112]), tensor([0.0065]), tensor([-0.0049]), tensor([0.0226]), tensor([0.0279]), tensor([0.0128]), tensor([-0.0121]), tensor([0.0182]), tensor([-0.0099]), tensor([0.0197]), tensor([0.0218]), tensor([-0.0033]), tensor([0.0144]), tensor([0.0112]), tensor([-0.0055]), tensor([0.0066]), tensor([0.0042]), tensor([0.0185]), tensor([-0.0125]), tensor([-0.0380]), tensor([0.0174]), tensor([-0.0035]), tensor([0.0276]), tensor([0.0426]), tensor([-0.0015]), tensor([-0.0116]), tensor([0.0387]), tensor([0.0081]), tensor([-0.0107]), tensor([0.0014]), tensor([0.0127]), tensor([0.0150]), tensor([0.0059]), tensor([0.0061]), tensor([0.0024]), tensor([-0.0086]), tensor([0.0259]), tensor([0.0126]), tensor([-0.0121]), tensor([0.0028]), tensor([0.0081])]\n"
     ]
    }
   ],
   "source": [
    "lstmonxlmroberta = LSTMOnXLMRoberta(768, 384, 1)\n",
    "lstmonxlmroberta.run()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
