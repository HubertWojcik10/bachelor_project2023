{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/germa/anaconda3/envs/project/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/var/folders/c6/k78nzngn7nbdd0786bxl3s0c0000gn/T/ipykernel_8264/4227447418.py:2: DeprecationWarning: \n",
      "Pyarrow will become a required dependency of pandas in the next major release of pandas (pandas 3.0),\n",
      "(to allow more performant data types, such as the Arrow string type, and better interoperability with other libraries)\n",
      "but was not found to be installed on your system.\n",
      "If this would cause problems for you,\n",
      "please provide us feedback at https://github.com/pandas-dev/pandas/issues/54466\n",
      "        \n",
      "  import pandas as pd\n"
     ]
    }
   ],
   "source": [
    "from transformers import XLMRobertaTokenizer, XLMRobertaModel\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "from torch import Tensor\n",
    "from typing import List, Tuple\n",
    "from torch.utils.data import TensorDataset, DataLoader, random_split\n",
    "import sys\n",
    "sys.path.append('/Users/germa/thesis/bachelor_project2023/src/models/lstm')\n",
    "from xlm_roberta import XLMRoberta\n",
    "sys.path.append('/Users/germa/thesis/bachelor_project2023/src/utils')\n",
    "from chunker import Chunker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMOnXLMRoberta(nn.Module):\n",
    "    def __init__(self, input_size, lstm_hidden_size, num_lstm_layers, model_name = 'xlm-roberta-base', train_path = '../../../data/train/train.csv', train_size = 0.8, batch_size = 32, shuffle = True):\n",
    "        super(LSTMOnXLMRoberta, self).__init__()\n",
    "        self.xlmroberta_model = XLMRoberta(model_name)\n",
    "        self.tokenizer = XLMRobertaTokenizer.from_pretrained(model_name)\n",
    "        self.chunker = Chunker(self.tokenizer, 512)\n",
    "        self.lstm = nn.LSTM(input_size=input_size,\n",
    "                            hidden_size=lstm_hidden_size,\n",
    "                            num_layers=num_lstm_layers,\n",
    "                            batch_first=True,\n",
    "                            bidirectional=False)\n",
    "        self.fc = nn.Linear(lstm_hidden_size * 2, 1)\n",
    "\n",
    "        parameters_to_optimize = list(self.fc.parameters()) + list(self.lstm.parameters()) + list(self.xlmroberta_model.parameters())\n",
    "\n",
    "        self.optimizer = torch.optim.AdamW(parameters_to_optimize, lr=1e-5)\n",
    "        self.loss_function = nn.MSELoss()\n",
    "        self.train_df= self.get_data(train_path)\n",
    "        torch.manual_seed(42)\n",
    "\n",
    "    def parameter_to_optimize(self):\n",
    "        for param in self.xlmroberta_model.parameters():\n",
    "            param.requires_grad = True\n",
    "        for param in self.lstm.parameters():\n",
    "            param.requires_grad = True\n",
    "        for param in self.fc.parameters():\n",
    "            param.requires_grad = True\n",
    "\n",
    "    def get_data(self, path):\n",
    "        train_df = pd.read_csv(path)\n",
    "        train_short_df = train_df.head(16)\n",
    "        return train_short_df#, train_df[\"overall\"]\n",
    "\n",
    "    def chunk_data(self, df):\n",
    "        texts1, texts2, labels = df[\"text1\"], df[\"text2\"], df[\"overall\"]\n",
    "        input_ids = []\n",
    "        for i in range(len(texts1)):\n",
    "            input_id_1 = self.chunker.chunk(texts1[i])\n",
    "            input_id_2 = self.chunker.chunk(texts2[i])\n",
    "            input_ids.append([input_id_1, input_id_2])           \n",
    "        return input_ids, labels\n",
    "    \n",
    "        \n",
    "    def _manage_device(self) -> None:\n",
    "        \"\"\"\n",
    "            Manage the device to run the model on\n",
    "        \"\"\"\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.xlmroberta_model.to(self.device)\n",
    "        self.lstm.to(self.device)\n",
    "        self.fc.to(self.device)\n",
    "    \n",
    "    def get_embeddings(self, input_ids):\n",
    "        \"\"\"\n",
    "            Get the embeddings from the XLM-Roberta model\n",
    "        \"\"\"\n",
    "        outputs = self.xlmroberta_model.run(input_ids)\n",
    "        return outputs\n",
    "\n",
    "\n",
    "    def pad_to_same_size(self, tensors):\n",
    "        max_size = max(tensor.size(0) for tensor in tensors)\n",
    "        padded_tensors = []\n",
    "        for tensor in tensors:\n",
    "            if tensor.size(0) < max_size:\n",
    "                padding = torch.zeros(max_size - tensor.size(0), tensor.size(1))\n",
    "                padded_tensor = torch.cat((tensor, padding), dim=0)\n",
    "                padded_tensors.append(padded_tensor)\n",
    "            else:\n",
    "                padded_tensors.append(tensor)\n",
    "        return torch.stack(padded_tensors)\n",
    "    \n",
    "    def pearson_correlation(self, labels_val, eval_loss):\n",
    "        print(f\"Labels: {labels_val}\")\n",
    "        print(f\"Eval loss: {eval_loss}\")\n",
    "\n",
    "        eval_loss= np.array([t.item() for t in eval_loss])\n",
    "        return np.corrcoef(labels_val, eval_loss)[0][1]\n",
    "\n",
    "    \n",
    "    def evaluate_model(self, input_ids_val, labels_val, batch_size = 4):\n",
    "        \"\"\"\n",
    "            Evaluate the model \"\"\" \n",
    "        self.eval()\n",
    "        output_val = []\n",
    "        with torch.no_grad():\n",
    "            for i in range(0, len(input_ids_val), batch_size):\n",
    "\n",
    "                input_batch_data = input_ids_val[i:i + batch_size]\n",
    "                input_batch = self.get_embeddings(input_batch_data)\n",
    "                label_batch = labels_val[i:i + batch_size]\n",
    "\n",
    "                for row, label in zip(input_batch, label_batch):\n",
    "                    index1, index2 = len(row[0]), len(row[1])\n",
    "                    row_padded = self.pad_to_same_size(row) \n",
    "                    lstm_out, _ = self.lstm(row_padded)\n",
    "                    lstm_out_last1 = lstm_out[0, index1 - 1, :]\n",
    "                    lstm_out_last2 = lstm_out[1, index2 - 1, :]\n",
    "                    nn = torch.cat((lstm_out_last1, lstm_out_last2), 0)\n",
    "                    output = self.fc(nn)\n",
    "                    output_val.append(output)\n",
    "                    \n",
    "        pearson = self.pearson_correlation(labels_val, output_val)\n",
    "        return pearson\n",
    "    \n",
    "    def train_model(self, input_ids_train, labels_train, input_ids_val, labels_val, batch_size = 4):\n",
    "        \"\"\"\n",
    "            Train the model\n",
    "        \"\"\"\n",
    "        best_pearson = -1\n",
    "        self.train()\n",
    "        for epoch in range(5):\n",
    "            print(f\"Epoch: {epoch}\")\n",
    "\n",
    "            for i in range(0, len(input_ids_train), batch_size):\n",
    "                print(f\"Batch: {i}\")\n",
    "                input_batch_data = input_ids_train[i:i + batch_size]\n",
    "                input_batch = self.get_embeddings(input_batch_data)\n",
    "                label_batch = labels_train[i:i + batch_size]\n",
    "                batch_loss = 0\n",
    "                self.optimizer.zero_grad()  # Clear gradients\n",
    "                \n",
    "                for row, label in zip(input_batch, label_batch):\n",
    "                    index1, index2 = len(row[0]), len(row[1])\n",
    "                    row_padded = self.pad_to_same_size(row) \n",
    "                    lstm_out, _ = self.lstm(row_padded)\n",
    "                    lstm_out_last1 = lstm_out[0, index1 - 1, :]\n",
    "                    lstm_out_last2 = lstm_out[1, index2 - 1, :]\n",
    "                    nn = torch.cat((lstm_out_last1, lstm_out_last2), 0)\n",
    "                    output = self.fc(nn)\n",
    "                    loss = self.loss_function(output, torch.tensor(label, dtype=torch.float32).view(1))\n",
    "                    batch_loss = batch_loss + loss\n",
    "    \n",
    "                batch_loss = batch_loss/batch_size \n",
    "                batch_loss.backward()  # Backpropagation \n",
    "                self.optimizer.step()  # Update weights\n",
    "                print(f\"Batch loss: {batch_loss}\")\n",
    "        \n",
    "            eval_pearson = self.evaluate_model(input_ids_val, labels_val)\n",
    "            print(f\"Eval pearson: {eval_pearson}\")\n",
    "            if eval_pearson > best_pearson:\n",
    "                best_pearson = eval_pearson\n",
    "                print(\"Saving the model...\")\n",
    "                torch.save(self.state_dict(), \"../../../saved_models/xlmroberta_on_lstm/best_lstm.pth\")\n",
    "\n",
    "    def run(self):\n",
    "        torch.autograd.set_detect_anomaly(True)\n",
    "        self._manage_device()\n",
    "        #shuffle data\n",
    "        train_df = self.train_df.sample(frac=1).reset_index(drop=True)\n",
    "        train_df, val_df = train_df[:int(len(train_df) * 0.8)], train_df[int(len(train_df) * 0.8):].reset_index(drop=True)\n",
    "        input_ids_train, labels_train = self.chunk_data(train_df)\n",
    "        input_ids_val, labels_val = self.chunk_data(val_df)\n",
    "        self.parameter_to_optimize()\n",
    "        self.train_model(input_ids_train, labels_train, input_ids_val, labels_val)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (777 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0\n",
      "Batch: 0\n",
      "Batch loss: 8.720625877380371\n",
      "Batch: 4\n",
      "Batch loss: 12.387468338012695\n",
      "Batch: 8\n",
      "Batch loss: 7.282646179199219\n",
      "Labels: 0    4.000000\n",
      "1    4.000000\n",
      "2    3.333333\n",
      "3    3.666667\n",
      "Name: overall, dtype: float64\n",
      "Eval loss: [tensor([0.0489]), tensor([0.0507]), tensor([0.0774]), tensor([0.0379])]\n",
      "Eval pearson: -0.6546647388128665\n",
      "Saving the model...\n",
      "Epoch: 1\n",
      "Batch: 0\n",
      "Batch loss: 8.402003288269043\n",
      "Batch: 4\n",
      "Batch loss: 12.090984344482422\n",
      "Batch: 8\n",
      "Batch loss: 7.172922134399414\n",
      "Labels: 0    4.000000\n",
      "1    4.000000\n",
      "2    3.333333\n",
      "3    3.666667\n",
      "Name: overall, dtype: float64\n",
      "Eval loss: [tensor([0.0853]), tensor([0.0860]), tensor([0.1163]), tensor([0.0803])]\n",
      "Eval pearson: -0.7847061049470134\n",
      "Epoch: 2\n",
      "Batch: 0\n",
      "Batch loss: 8.21849536895752\n",
      "Batch: 4\n",
      "Batch loss: 11.889848709106445\n",
      "Batch: 8\n",
      "Batch loss: 7.013045310974121\n",
      "Labels: 0    4.000000\n",
      "1    4.000000\n",
      "2    3.333333\n",
      "3    3.666667\n",
      "Name: overall, dtype: float64\n",
      "Eval loss: [tensor([0.1253]), tensor([0.1317]), tensor([0.1669]), tensor([0.1263])]\n",
      "Eval pearson: -0.8365796727864312\n",
      "Epoch: 3\n",
      "Batch: 0\n",
      "Batch loss: 7.9392242431640625\n",
      "Batch: 4\n",
      "Batch loss: 11.530542373657227\n",
      "Batch: 8\n",
      "Batch loss: 6.749061584472656\n",
      "Labels: 0    4.000000\n",
      "1    4.000000\n",
      "2    3.333333\n",
      "3    3.666667\n",
      "Name: overall, dtype: float64\n",
      "Eval loss: [tensor([0.2180]), tensor([0.2498]), tensor([0.2512]), tensor([0.2167])]\n",
      "Eval pearson: -0.3152741176566108\n",
      "Saving the model...\n",
      "Epoch: 4\n",
      "Batch: 0\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m lstmonxlmroberta \u001b[38;5;241m=\u001b[39m LSTMOnXLMRoberta(\u001b[38;5;241m768\u001b[39m, \u001b[38;5;241m384\u001b[39m, \u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m----> 2\u001b[0m \u001b[43mlstmonxlmroberta\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[13], line 155\u001b[0m, in \u001b[0;36mLSTMOnXLMRoberta.run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    153\u001b[0m input_ids_val, labels_val \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchunk_data(val_df)\n\u001b[1;32m    154\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparameter_to_optimize()\n\u001b[0;32m--> 155\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_ids_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels_val\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[13], line 135\u001b[0m, in \u001b[0;36mLSTMOnXLMRoberta.train_model\u001b[0;34m(self, input_ids_train, labels_train, input_ids_val, labels_val, batch_size)\u001b[0m\n\u001b[1;32m    132\u001b[0m     batch_loss \u001b[38;5;241m=\u001b[39m batch_loss \u001b[38;5;241m+\u001b[39m loss\n\u001b[1;32m    134\u001b[0m batch_loss \u001b[38;5;241m=\u001b[39m batch_loss\u001b[38;5;241m/\u001b[39mbatch_size \n\u001b[0;32m--> 135\u001b[0m \u001b[43mbatch_loss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Backpropagation \u001b[39;00m\n\u001b[1;32m    136\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mstep()  \u001b[38;5;66;03m# Update weights\u001b[39;00m\n\u001b[1;32m    137\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBatch loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbatch_loss\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/project/lib/python3.12/site-packages/torch/_tensor.py:522\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    512\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    513\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    514\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    515\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    520\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    521\u001b[0m     )\n\u001b[0;32m--> 522\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    523\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    524\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/project/lib/python3.12/site-packages/torch/autograd/__init__.py:266\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    261\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    263\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    264\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    265\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 266\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    267\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    272\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    273\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    274\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "lstmonxlmroberta = LSTMOnXLMRoberta(768, 384, 1)\n",
    "lstmonxlmroberta.run()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
