{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from transformers import XLMRobertaModel, XLMRobertaTokenizer\n",
    "import torch\n",
    "from torch import Tensor\n",
    "\n",
    "tokenizer = XLMRobertaTokenizer.from_pretrained(\"xlm-roberta-base\", pad_token=\"<pad>\")\n",
    "model = XLMRobertaModel.from_pretrained(\"xlm-roberta-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_csv(\"../../data/train/train.csv\")\n",
    "test_data = pd.read_csv(\"../../data/test/final_test_pairs.csv\")\n",
    "\n",
    "print(train_data.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_embedding(text: Tensor, target_length: int) -> Tensor:\n",
    "    \"\"\"\n",
    "    Pad the input text with ones to reach the target length of a sequence (sentence).\n",
    "    \n",
    "    Args:\n",
    "        text (Tensor): The input text tensor.\n",
    "        target_length (int): The desired length of the padded text tensor.\n",
    "        \n",
    "    Returns:\n",
    "        Tensor: The padded text tensor.\n",
    "    \"\"\"\n",
    "    text = torch.tensor(text)\n",
    "\n",
    "    # add ones to the end of the text (1 is the padding token)\n",
    "    text = torch.nn.functional.pad(text, (0, target_length - text.shape[1]), 'constant', 1)\n",
    "\n",
    "    return text\n",
    "\n",
    "def tokenize_and_shorten_sentence(text: str) -> Tensor:\n",
    "    \"\"\"\n",
    "    Tokenize the input text and shorten it to 256 tokens.\n",
    "\n",
    "    Args:\n",
    "        text (str): The input text.\n",
    "\n",
    "    Returns:\n",
    "        Tensor: The tokenized and shortened text tensor.\n",
    "    \"\"\"\n",
    "\n",
    "    tokenized_text = tokenizer(text, return_tensors=\"pt\", padding=False, truncation=False) #padding = False and truncation = False to get the exact length of the text\n",
    "    if tokenized_text[\"input_ids\"].shape[1] > 256:\n",
    "        # TODO: decide where to truncate the text, meaning how many tokens to keep from the head and how many from the tail\n",
    "        # note: the model has a max length of 512 tokens and we need to keep the [CLS] ? and [SEP] tokens\n",
    "        tokenized_text[\"shorten_ids\"] = torch.cat((tokenized_text[\"input_ids\"][:, :200], tokenized_text[\"input_ids\"][:, -55:]), dim=1)\n",
    "    else:\n",
    "        tokenized_text[\"shorten_ids\"] = tokenizer(text, return_tensors=\"pt\", padding=\"max_length\", truncation=False, max_length=256)[\"input_ids\"]\n",
    "        \n",
    "    return tokenized_text[\"shorten_ids\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM = 1 #note: len(train_data)\n",
    "#train_embeddings = torch.empty((NUM, 512, 768)) # note: if we want to keep the whole sequence's embeddings\n",
    "train_embeddings = torch.empty((NUM, 768)) # note: take into account the [CLS] token only as it represents the whole sentence\n",
    "\n",
    "sep_token = torch.tensor([tokenizer.sep_token_id]).unsqueeze(0)\n",
    "\n",
    "for i in range(len(train_data[:NUM])):\n",
    "    text1 = train_data[\"text1\"][i]\n",
    "    text2 = train_data[\"text2\"][i]\n",
    "    text1_tokenized = tokenize_and_shorten_sentence(text1)\n",
    "    text2_tokenized = tokenize_and_shorten_sentence(text2)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        text = torch.cat((text1_tokenized, sep_token, text2_tokenized), dim=1)\n",
    "        # TODO: investigate the attention mask\n",
    "        text = {\"input_ids\": text, \"attention_mask\": torch.ones(text.shape)}\n",
    "        outputs = model(**text)\n",
    "        embeddings = outputs.last_hidden_state\n",
    "        embeddings = pad_embedding(embeddings, 512)\n",
    "\n",
    "        cls_token_embedding = outputs.pooler_output\n",
    "        train_embeddings[i] = cls_token_embedding"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "thesis_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
