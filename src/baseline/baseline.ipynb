{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from transformers import XLMRobertaForSequenceClassification, XLMRobertaTokenizer, XLMRobertaConfig\n",
    "import torch\n",
    "from torch import Tensor\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import numpy as np\n",
    "\n",
    "tokenizer = XLMRobertaTokenizer.from_pretrained(\"xlm-roberta-base\")\n",
    "config = XLMRobertaConfig(\n",
    "    num_labels=1,\n",
    "    output_hidden_states=False,\n",
    "    output_attentions=False,\n",
    ")\n",
    "config.vocab_size = tokenizer.vocab_size\n",
    "\n",
    "model = XLMRobertaForSequenceClassification.from_pretrained(\"xlm-roberta-base\", num_labels=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XLMRobertaForSequenceClassification(\n",
       "  (roberta): XLMRobertaModel(\n",
       "    (embeddings): XLMRobertaEmbeddings(\n",
       "      (word_embeddings): Embedding(250002, 768, padding_idx=1)\n",
       "      (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
       "      (token_type_embeddings): Embedding(1, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): XLMRobertaEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x XLMRobertaLayer(\n",
       "          (attention): XLMRobertaAttention(\n",
       "            (self): XLMRobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): XLMRobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): XLMRobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): XLMRobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (classifier): XLMRobertaClassificationHead(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "    (out_proj): Linear(in_features=768, out_features=1, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Unnamed: 0', 'pair_id', 'id1', 'id2', 'text1', 'text2', 'overall',\n",
      "       'lang1', 'lang2'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "train_data = pd.read_csv(\"../../data/train/train.csv\")\n",
    "test_data = pd.read_csv(\"../../data/test/final_test_pairs.csv\")\n",
    "\n",
    "print(train_data.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_and_shorten_sentence(text: str) -> Tensor:\n",
    "    \"\"\"\n",
    "    Tokenize the input text and shorten it to 256 tokens.\n",
    "\n",
    "    Args:\n",
    "        text (str): The input text.\n",
    "\n",
    "    Returns:\n",
    "        Tensor: The tokenized and shortened text tensor.\n",
    "    \"\"\"\n",
    "    tokenized_text = tokenizer(text, return_tensors=\"pt\", padding=False, truncation=False, add_special_tokens=False, max_length = None)\n",
    "\n",
    "    if tokenized_text[\"input_ids\"].shape[1] > 256:\n",
    "        shorten_ids =  tokenized_text[\"input_ids\"][:, :200].tolist()[0] + tokenized_text[\"input_ids\"][:, -54:].tolist()[0]\n",
    "    else:\n",
    "        shorten_ids = tokenized_text[\"input_ids\"].tolist()[0] + [tokenizer.pad_token_id] * (254 - tokenized_text[\"input_ids\"].shape[1])\n",
    "\n",
    "\n",
    "    return tokenizer.decode(shorten_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_texts(text1:str, text2:str):\n",
    "    tokenized_text = tokenizer(text1, text2, return_tensors=\"pt\", padding=\"max_length\", \n",
    "                               truncation=True, add_special_tokens=True, max_length=512)\n",
    "    return tokenized_text[\"input_ids\"], tokenized_text[\"attention_mask\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids = []\n",
    "attention_mask = []\n",
    "\n",
    "\n",
    "for i in range(len(train_data[:25])):\n",
    "    text1 = train_data[\"text1\"][i]\n",
    "    text2 = train_data[\"text2\"][i]\n",
    "    text1_truncated = tokenize_and_shorten_sentence(text1)\n",
    "    text2_truncated = tokenize_and_shorten_sentence(text2)\n",
    "\n",
    "    text_input_ids, text_attention_mask = tokenize_texts(text1_truncated, text2_truncated)\n",
    "    input_ids.append(text_input_ids.tolist()[0])\n",
    "    attention_mask.append(text_attention_mask.tolist()[0])\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "[0, 30607, 28960, 136, 1884, 1821, 12, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 62, 59714, 9, 66671, 6048, 35011, 15889, 1419, 2750, 13028, 100, 7082, 43953, 13, 23, 52888, 23, 780, 509, 34784, 297, 68318, 100, 213796, 10366, 4, 85727, 95319, 4, 136, 139355, 111, 10, 11476, 56859, 11814, 20271, 10, 46824, 5, 212840, 223, 6678, 360, 686, 24392, 4, 4039, 4, 509, 373, 14437, 23, 52888, 98, 10, 183594, 1295, 25134, 96220, 4, 207048, 2804, 5, 581, 170420, 111, 70, 46824, 3542, 959, 109312, 121447, 5, 1529, 509, 8035, 34658, 15490, 97910, 64227, 5, 4153, 686, 24392, 4, 142, 121839, 9, 77007, 183093, 4, 3671, 86, 2822, 15889, 2450, 35743, 20271, 1919, 51, 226534, 7, 7844, 45880, 47, 66801, 38591, 123099, 31, 237, 10, 20846, 92812, 25469, 13, 23, 6360, 397, 53089, 70, 165523, 80399, 4, 764, 63043, 297, 764, 14865, 46526, 56, 297, 10, 738, 257, 136, 121952, 6863, 59714, 237, 2831, 111, 10, 2070, 66, 68869, 5, 13025, 26, 7, 2843, 22299, 297, 390, 35011, 15889, 1419, 22758, 196523, 678, 21, 38543, 70, 137374, 100, 70, 103494, 538, 208984, 143107, 23, 79602, 7, 10745, 4, 118623, 4, 23, 5276, 4153, 686, 24392, 1556, 8, 93, 297, 261, 67666, 2320, 678, 196523, 136, 70, 208984, 7, 99, 70, 143107, 4, 1284, 196523, 22299, 297, 4049, 678, 144888, 214, 70, 52, 510, 7831, 63, 50155, 70, 143107, 5, 194583, 2795, 3336, 90, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2]\n",
      "512\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "Size mismatch between tensors",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 6\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mlen\u001b[39m(attention_mask[\u001b[38;5;241m22\u001b[39m]))\n\u001b[1;32m      5\u001b[0m score \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(train_data[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moverall\u001b[39m\u001b[38;5;124m\"\u001b[39m])\u001b[38;5;241m.\u001b[39mfloat()\n\u001b[0;32m----> 6\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[43mTensorDataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlong\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlong\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscore\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mview\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/thesis_env/lib/python3.9/site-packages/torch/utils/data/dataset.py:204\u001b[0m, in \u001b[0;36mTensorDataset.__init__\u001b[0;34m(self, *tensors)\u001b[0m\n\u001b[1;32m    203\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39mtensors: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 204\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mall\u001b[39m(tensors[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m) \u001b[38;5;241m==\u001b[39m tensor\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m tensor \u001b[38;5;129;01min\u001b[39;00m tensors), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSize mismatch between tensors\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    205\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtensors \u001b[38;5;241m=\u001b[39m tensors\n",
      "\u001b[0;31mAssertionError\u001b[0m: Size mismatch between tensors"
     ]
    }
   ],
   "source": [
    "print(attention_mask[22])\n",
    "print(input_ids[22])\n",
    "print(len(attention_mask[22]))\n",
    "\n",
    "score = torch.tensor(train_data[\"overall\"]).float()\n",
    "data = TensorDataset(torch.tensor(input_ids).long(), torch.tensor(attention_mask).long(), score.view(-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = int(0.8 * len(data))\n",
    "test_size = len(data) - train_size\n",
    "train_dataset, val_dataset = torch.utils.data.random_split(data, [train_size, test_size])\n",
    "\n",
    "loader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=8, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_pearson_corr(model, val_loader):\n",
    "    \"\"\"\n",
    "    Evaluate the Pearson correlation of the model.\n",
    "\n",
    "    Args:\n",
    "        model (XLMRobertaForSequenceClassification): The model to evaluate.\n",
    "        val_loader (DataLoader): The validation data loader.\n",
    "\n",
    "    Returns:\n",
    "        float: The Pearson correlation of the model.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    preds = []\n",
    "    labels = []\n",
    "    for idx, batch in enumerate(val_loader):\n",
    "        input_ids, attention_mask, label = tuple(t.to(device) for t in batch)\n",
    "        with torch.no_grad():\n",
    "            outputs = model(input_ids, attention_mask=attention_mask, labels=None)\n",
    "            preds.append(outputs.logits.cpu().numpy())\n",
    "            labels.append(label.cpu().numpy())\n",
    "            print(f\"The correlation for batch number {len(idx)} is :\")\n",
    "            print(np.corrcoef(np.concatenate(preds).flatten(), np.concatenate(labels).flatten())[0, 1])\n",
    "    preds = np.concatenate(preds)\n",
    "    labels = np.concatenate(labels)\n",
    "    return np.corrcoef(preds.flatten(), labels.flatten())[0, 1]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average training loss: 6.33\n",
      "current loss: 6.329257965087891\n",
      "logits: tensor([[ 0.0171],\n",
      "        [-0.0023],\n",
      "        [-0.0311],\n",
      "        [-0.1140],\n",
      "        [ 0.2167],\n",
      "        [ 0.0257],\n",
      "        [-0.0162],\n",
      "        [ 0.2933]], grad_fn=<AddmmBackward0>)\n",
      "average training loss: 7.90\n",
      "current loss: 6.7416276931762695\n",
      "logits: tensor([[0.8825],\n",
      "        [0.2642],\n",
      "        [1.2874],\n",
      "        [1.0074],\n",
      "        [0.8416],\n",
      "        [1.1070],\n",
      "        [0.5024],\n",
      "        [0.5320]], grad_fn=<AddmmBackward0>)\n",
      "average training loss: 6.00\n",
      "current loss: 2.5527827739715576\n",
      "logits: tensor([[1.6781],\n",
      "        [1.9889],\n",
      "        [2.2216],\n",
      "        [2.1277],\n",
      "        [1.5780],\n",
      "        [1.8669],\n",
      "        [1.8939],\n",
      "        [1.8594]], grad_fn=<AddmmBackward0>)\n",
      "average training loss: 4.71\n",
      "current loss: 2.4748387336730957\n",
      "logits: tensor([[2.7295],\n",
      "        [2.9217],\n",
      "        [3.2007],\n",
      "        [2.4448],\n",
      "        [0.5208],\n",
      "        [3.1640],\n",
      "        [2.2395],\n",
      "        [2.8676]], grad_fn=<AddmmBackward0>)\n",
      "average training loss: 3.99\n",
      "current loss: 1.955312967300415\n",
      "logits: tensor([[2.7895],\n",
      "        [3.1493],\n",
      "        [3.0286],\n",
      "        [2.1083],\n",
      "        [3.3318],\n",
      "        [2.9885],\n",
      "        [3.1626],\n",
      "        [3.5898]], grad_fn=<AddmmBackward0>)\n",
      "average training loss: 3.64\n",
      "current loss: 2.128446102142334\n",
      "logits: tensor([[2.4584],\n",
      "        [0.6586],\n",
      "        [3.0372],\n",
      "        [2.3645],\n",
      "        [3.2871],\n",
      "        [3.2318],\n",
      "        [2.7943],\n",
      "        [1.4212]], grad_fn=<AddmmBackward0>)\n",
      "average training loss: 3.36\n",
      "current loss: 2.2994985580444336\n",
      "logits: tensor([[3.2998],\n",
      "        [2.6014],\n",
      "        [3.1550],\n",
      "        [2.5267],\n",
      "        [2.1861],\n",
      "        [2.9139],\n",
      "        [2.9071],\n",
      "        [2.3940]], grad_fn=<AddmmBackward0>)\n",
      "average training loss: 3.11\n",
      "current loss: 1.5635322332382202\n",
      "logits: tensor([[2.9652],\n",
      "        [2.1122],\n",
      "        [2.8681],\n",
      "        [1.7739],\n",
      "        [2.8496],\n",
      "        [2.5680],\n",
      "        [2.6419],\n",
      "        [2.0303]], grad_fn=<AddmmBackward0>)\n",
      "average training loss: 2.95\n",
      "current loss: 2.3304054737091064\n",
      "logits: tensor([[2.7407],\n",
      "        [2.8440],\n",
      "        [2.8294],\n",
      "        [2.9891],\n",
      "        [3.0067],\n",
      "        [2.8538],\n",
      "        [2.8247],\n",
      "        [0.9842]], grad_fn=<AddmmBackward0>)\n",
      "average training loss: 2.81\n",
      "current loss: 1.6217986345291138\n",
      "logits: tensor([[3.1283],\n",
      "        [3.2157],\n",
      "        [2.1556],\n",
      "        [2.3349],\n",
      "        [2.6156],\n",
      "        [2.6467],\n",
      "        [3.1853],\n",
      "        [2.9054]], grad_fn=<AddmmBackward0>)\n",
      "average training loss: 2.68\n",
      "current loss: 0.7927518486976624\n",
      "logits: tensor([[2.7879],\n",
      "        [2.3336],\n",
      "        [2.5600],\n",
      "        [3.0186],\n",
      "        [2.1610],\n",
      "        [3.4707],\n",
      "        [2.9005],\n",
      "        [2.0949]], grad_fn=<AddmmBackward0>)\n",
      "average training loss: 2.58\n",
      "current loss: 1.6958675384521484\n",
      "logits: tensor([[2.7898],\n",
      "        [2.8141],\n",
      "        [3.2713],\n",
      "        [3.0994],\n",
      "        [3.0474],\n",
      "        [2.8426],\n",
      "        [2.8414],\n",
      "        [3.0568]], grad_fn=<AddmmBackward0>)\n",
      "average training loss: 2.48\n",
      "current loss: 1.1361769437789917\n",
      "logits: tensor([[2.5144],\n",
      "        [2.7611],\n",
      "        [1.9710],\n",
      "        [2.5060],\n",
      "        [2.2427],\n",
      "        [2.9025],\n",
      "        [2.0796],\n",
      "        [3.0914]], grad_fn=<AddmmBackward0>)\n",
      "average training loss: 2.37\n",
      "current loss: 1.3820562362670898\n",
      "logits: tensor([[2.9398],\n",
      "        [3.2216],\n",
      "        [3.2301],\n",
      "        [2.9685],\n",
      "        [3.0640],\n",
      "        [2.8416],\n",
      "        [2.8525],\n",
      "        [2.0255]], grad_fn=<AddmmBackward0>)\n",
      "average training loss: 2.33\n",
      "current loss: 2.0455989837646484\n",
      "logits: tensor([[3.0847],\n",
      "        [3.1486],\n",
      "        [2.2038],\n",
      "        [1.9387],\n",
      "        [3.2126],\n",
      "        [2.7493],\n",
      "        [2.8289],\n",
      "        [3.1814]], grad_fn=<AddmmBackward0>)\n",
      "average training loss: 2.26\n",
      "current loss: 1.2789701223373413\n",
      "logits: tensor([[2.7277],\n",
      "        [3.1661],\n",
      "        [2.4044],\n",
      "        [2.8576],\n",
      "        [3.0758],\n",
      "        [2.9342],\n",
      "        [2.7454],\n",
      "        [3.1030]], grad_fn=<AddmmBackward0>)\n",
      "average training loss: 2.21\n",
      "current loss: 1.2191886901855469\n",
      "logits: tensor([[2.0913],\n",
      "        [2.7764],\n",
      "        [2.9173],\n",
      "        [2.9338],\n",
      "        [2.3155],\n",
      "        [3.1534],\n",
      "        [2.4743],\n",
      "        [2.8935]], grad_fn=<AddmmBackward0>)\n",
      "average training loss: 2.15\n",
      "current loss: 2.0746817588806152\n",
      "logits: tensor([[2.8663],\n",
      "        [2.8941],\n",
      "        [2.8718],\n",
      "        [2.7250],\n",
      "        [2.8539],\n",
      "        [2.3046],\n",
      "        [2.8962],\n",
      "        [2.3200]], grad_fn=<AddmmBackward0>)\n",
      "average training loss: 2.11\n",
      "current loss: 0.69804847240448\n",
      "logits: tensor([[2.4400],\n",
      "        [3.3706],\n",
      "        [2.8443],\n",
      "        [2.8707],\n",
      "        [2.7534],\n",
      "        [2.9442],\n",
      "        [2.9609],\n",
      "        [2.9252]], grad_fn=<AddmmBackward0>)\n",
      "average training loss: 2.09\n",
      "current loss: 1.1396374702453613\n",
      "logits: tensor([[2.6450],\n",
      "        [2.4319],\n",
      "        [2.0410],\n",
      "        [2.7228],\n",
      "        [3.0272],\n",
      "        [2.7594],\n",
      "        [2.5898],\n",
      "        [2.8640]], grad_fn=<AddmmBackward0>)\n",
      "average training loss: 2.04\n",
      "current loss: 1.2562923431396484\n",
      "logits: tensor([[2.7102],\n",
      "        [2.8470],\n",
      "        [2.8758],\n",
      "        [2.7419],\n",
      "        [2.8528],\n",
      "        [2.9397],\n",
      "        [2.6784],\n",
      "        [2.8712]], grad_fn=<AddmmBackward0>)\n",
      "average training loss: 2.01\n",
      "current loss: 1.2219949960708618\n",
      "logits: tensor([[2.7656],\n",
      "        [3.0153],\n",
      "        [2.0851],\n",
      "        [2.4413],\n",
      "        [3.1198],\n",
      "        [2.3113],\n",
      "        [2.7549],\n",
      "        [2.6119]], grad_fn=<AddmmBackward0>)\n",
      "average training loss: 1.97\n",
      "current loss: 1.045345425605774\n",
      "logits: tensor([[2.1968],\n",
      "        [2.1014],\n",
      "        [2.0418],\n",
      "        [2.5639],\n",
      "        [2.7994],\n",
      "        [3.1570],\n",
      "        [2.5757],\n",
      "        [2.6957]], grad_fn=<AddmmBackward0>)\n",
      "average training loss: 1.95\n",
      "current loss: 1.0120477676391602\n",
      "logits: tensor([[3.1670],\n",
      "        [3.0045],\n",
      "        [2.4190],\n",
      "        [3.0683],\n",
      "        [2.6066],\n",
      "        [2.4967],\n",
      "        [2.8256],\n",
      "        [2.9267]], grad_fn=<AddmmBackward0>)\n",
      "average training loss: 1.93\n",
      "current loss: 1.3072562217712402\n",
      "logits: tensor([[2.3195],\n",
      "        [2.1473],\n",
      "        [2.6740],\n",
      "        [2.5065],\n",
      "        [2.3409],\n",
      "        [3.0958],\n",
      "        [2.9555],\n",
      "        [2.5253]], grad_fn=<AddmmBackward0>)\n",
      "average training loss: 1.90\n",
      "current loss: 1.6837596893310547\n",
      "logits: tensor([[3.1203],\n",
      "        [3.0266],\n",
      "        [3.0772],\n",
      "        [2.0514],\n",
      "        [2.9765],\n",
      "        [2.9879],\n",
      "        [3.0572],\n",
      "        [3.3054]], grad_fn=<AddmmBackward0>)\n",
      "average training loss: 1.88\n",
      "current loss: 1.2004932165145874\n",
      "logits: tensor([[1.9201],\n",
      "        [2.0022],\n",
      "        [2.7788],\n",
      "        [2.9639],\n",
      "        [2.3938],\n",
      "        [3.1645],\n",
      "        [2.6834],\n",
      "        [1.6430]], grad_fn=<AddmmBackward0>)\n",
      "average training loss: 1.85\n",
      "current loss: 0.43042826652526855\n",
      "logits: tensor([[2.1478],\n",
      "        [2.3837],\n",
      "        [3.1130],\n",
      "        [2.0195],\n",
      "        [2.7300],\n",
      "        [2.7744],\n",
      "        [2.6964],\n",
      "        [3.0813]], grad_fn=<AddmmBackward0>)\n",
      "average training loss: 1.82\n",
      "current loss: 1.0983538627624512\n",
      "logits: tensor([[3.1938],\n",
      "        [3.2036],\n",
      "        [3.1478],\n",
      "        [3.2632],\n",
      "        [3.0681],\n",
      "        [3.2188],\n",
      "        [3.3395],\n",
      "        [3.1526]], grad_fn=<AddmmBackward0>)\n",
      "average training loss: 1.79\n",
      "current loss: 0.9074592590332031\n",
      "logits: tensor([[3.2300],\n",
      "        [2.5389],\n",
      "        [3.3689],\n",
      "        [2.8421],\n",
      "        [2.6807],\n",
      "        [2.3674],\n",
      "        [3.2596],\n",
      "        [3.4149]], grad_fn=<AddmmBackward0>)\n",
      "average training loss: 1.77\n",
      "current loss: 1.7423123121261597\n",
      "logits: tensor([[3.3658],\n",
      "        [3.4629],\n",
      "        [3.1501],\n",
      "        [3.3635],\n",
      "        [2.8868],\n",
      "        [2.9785],\n",
      "        [3.5888],\n",
      "        [3.2767]], grad_fn=<AddmmBackward0>)\n",
      "average training loss: 1.76\n",
      "current loss: 0.9691652655601501\n",
      "logits: tensor([[2.8496],\n",
      "        [2.0142],\n",
      "        [3.2688],\n",
      "        [2.7170],\n",
      "        [2.1167],\n",
      "        [3.2547],\n",
      "        [3.0166],\n",
      "        [1.6724]], grad_fn=<AddmmBackward0>)\n",
      "average training loss: 1.75\n",
      "current loss: 2.137767791748047\n",
      "logits: tensor([[3.0639],\n",
      "        [1.7947],\n",
      "        [2.9417],\n",
      "        [2.2680],\n",
      "        [2.2306],\n",
      "        [2.5719],\n",
      "        [2.9956],\n",
      "        [2.7851]], grad_fn=<AddmmBackward0>)\n",
      "average training loss: 1.73\n",
      "current loss: 1.686032772064209\n",
      "logits: tensor([[3.0846],\n",
      "        [2.8378],\n",
      "        [2.5341],\n",
      "        [2.8895],\n",
      "        [2.9604],\n",
      "        [2.7724],\n",
      "        [2.8837],\n",
      "        [2.9506]], grad_fn=<AddmmBackward0>)\n",
      "average training loss: 1.71\n",
      "current loss: 0.674371600151062\n",
      "logits: tensor([[2.2357],\n",
      "        [2.1663],\n",
      "        [2.5936],\n",
      "        [1.5474],\n",
      "        [3.0623],\n",
      "        [3.0209],\n",
      "        [1.8726],\n",
      "        [2.7565]], grad_fn=<AddmmBackward0>)\n",
      "average training loss: 1.68\n",
      "current loss: 0.6964717507362366\n",
      "logits: tensor([[3.1919],\n",
      "        [3.4274],\n",
      "        [3.2813],\n",
      "        [2.9001],\n",
      "        [3.4048],\n",
      "        [3.3148],\n",
      "        [1.6836],\n",
      "        [3.2909]], grad_fn=<AddmmBackward0>)\n",
      "average training loss: 1.67\n",
      "current loss: 0.27581173181533813\n",
      "logits: tensor([[1.4675],\n",
      "        [3.0374],\n",
      "        [1.9581],\n",
      "        [3.2398],\n",
      "        [1.5077],\n",
      "        [3.4188],\n",
      "        [3.5442],\n",
      "        [3.2232]], grad_fn=<AddmmBackward0>)\n",
      "average training loss: 1.65\n",
      "current loss: 2.2304086685180664\n",
      "logits: tensor([[3.2588],\n",
      "        [3.4386],\n",
      "        [3.0159],\n",
      "        [1.7010],\n",
      "        [3.2323],\n",
      "        [2.9826],\n",
      "        [2.0255],\n",
      "        [3.2002]], grad_fn=<AddmmBackward0>)\n",
      "average training loss: 1.64\n",
      "current loss: 0.7311919331550598\n",
      "logits: tensor([[3.2152],\n",
      "        [3.1411],\n",
      "        [3.6407],\n",
      "        [3.3637],\n",
      "        [3.1772],\n",
      "        [2.6463],\n",
      "        [3.1446],\n",
      "        [2.3767]], grad_fn=<AddmmBackward0>)\n",
      "average training loss: 1.62\n",
      "current loss: 0.5113977789878845\n",
      "logits: tensor([[1.6220],\n",
      "        [1.2514],\n",
      "        [2.6705],\n",
      "        [2.9816],\n",
      "        [3.5564],\n",
      "        [3.3727],\n",
      "        [3.5340],\n",
      "        [3.4382]], grad_fn=<AddmmBackward0>)\n",
      "average training loss: 1.60\n",
      "current loss: 0.952791690826416\n",
      "logits: tensor([[1.6119],\n",
      "        [3.1915],\n",
      "        [3.2549],\n",
      "        [2.5646],\n",
      "        [3.1043],\n",
      "        [2.9914],\n",
      "        [3.1471],\n",
      "        [3.4085]], grad_fn=<AddmmBackward0>)\n",
      "average training loss: 1.60\n",
      "current loss: 0.47115203738212585\n",
      "logits: tensor([[3.3522],\n",
      "        [3.4026],\n",
      "        [2.1857],\n",
      "        [3.4148],\n",
      "        [3.0479],\n",
      "        [3.3807],\n",
      "        [3.2905],\n",
      "        [3.5723]], grad_fn=<AddmmBackward0>)\n",
      "average training loss: 1.58\n",
      "current loss: 1.0958971977233887\n",
      "logits: tensor([[2.5930],\n",
      "        [2.4075],\n",
      "        [2.9482],\n",
      "        [3.5311],\n",
      "        [3.2329],\n",
      "        [2.2124],\n",
      "        [1.8043],\n",
      "        [3.2188]], grad_fn=<AddmmBackward0>)\n",
      "average training loss: 1.56\n",
      "current loss: 1.5054773092269897\n",
      "logits: tensor([[3.3528],\n",
      "        [1.5957],\n",
      "        [3.3087],\n",
      "        [1.6427],\n",
      "        [1.6922],\n",
      "        [1.4674],\n",
      "        [2.2322],\n",
      "        [3.5627]], grad_fn=<AddmmBackward0>)\n",
      "average training loss: 1.55\n",
      "current loss: 0.5272443294525146\n",
      "logits: tensor([[3.1903],\n",
      "        [2.5264],\n",
      "        [2.9561],\n",
      "        [1.9797],\n",
      "        [3.3560],\n",
      "        [2.7629],\n",
      "        [3.1375],\n",
      "        [3.2974]], grad_fn=<AddmmBackward0>)\n",
      "average training loss: 1.53\n",
      "current loss: 0.5248556137084961\n",
      "logits: tensor([[3.4017],\n",
      "        [2.7287],\n",
      "        [2.1733],\n",
      "        [2.0414],\n",
      "        [3.6350],\n",
      "        [1.8179],\n",
      "        [3.5887],\n",
      "        [2.8608]], grad_fn=<AddmmBackward0>)\n",
      "average training loss: 1.51\n",
      "current loss: 0.5135494470596313\n",
      "logits: tensor([[3.5522],\n",
      "        [3.0434],\n",
      "        [3.7296],\n",
      "        [3.1974],\n",
      "        [1.5782],\n",
      "        [2.2161],\n",
      "        [3.2251],\n",
      "        [3.8457]], grad_fn=<AddmmBackward0>)\n",
      "average training loss: 1.50\n",
      "current loss: 0.73283851146698\n",
      "logits: tensor([[3.5059],\n",
      "        [3.1434],\n",
      "        [1.8510],\n",
      "        [3.0307],\n",
      "        [3.3651],\n",
      "        [1.3232],\n",
      "        [3.6698],\n",
      "        [3.4431]], grad_fn=<AddmmBackward0>)\n",
      "average training loss: 1.49\n",
      "current loss: 0.6214764714241028\n",
      "logits: tensor([[1.4451],\n",
      "        [2.9632],\n",
      "        [1.7517],\n",
      "        [1.9936],\n",
      "        [1.4986],\n",
      "        [3.0521],\n",
      "        [3.5530],\n",
      "        [3.6776]], grad_fn=<AddmmBackward0>)\n",
      "average training loss: 719.71\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "evaluation() takes 2 positional arguments but 3 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 33\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maverage training loss: \u001b[39m\u001b[38;5;132;01m{0:.2f}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(total_loss))\n\u001b[1;32m     32\u001b[0m \u001b[38;5;66;03m#perfom compute_accuracy on validation set\u001b[39;00m\n\u001b[0;32m---> 33\u001b[0m \u001b[43mevaluation\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mTypeError\u001b[0m: evaluation() takes 2 positional arguments but 3 were given"
     ]
    }
   ],
   "source": [
    "EPOCHS = 1\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-5)\n",
    "total_loss = 0\n",
    "loss_values = []\n",
    "\n",
    "def train(model, loader, val_loader, optimizer, EPOCHS):\n",
    "\n",
    "   model.train()\n",
    "   for epoch in range(EPOCHS):\n",
    "      for idx, (ids, att, val) in enumerate(loader):\n",
    "         ids, att, val = ids.to(device), att.to(device), val.to(device)\n",
    "         outputs = model(input_ids=ids, attention_mask=att, labels=val)\n",
    "         loss, logits = outputs[:2]\n",
    "         optimizer.zero_grad()\n",
    "         loss.backward()\n",
    "         optimizer.step()\n",
    "         total_loss += loss.item()\n",
    "         torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "\n",
    "         if idx % 10 == 0:\n",
    "            print(\"average training loss: {0:.2f}\".format(total_loss / (idx+1)))\n",
    "            print(\"current loss:\", loss.item())\n",
    "            print(f\"logits: {logits}\")\n",
    "            \n",
    "      loss_values.append(total_loss)\n",
    "      print(\"average training loss: {0:.2f}\".format(total_loss))\n",
    "\n",
    "train(model, loader, val_loader, optimizer, EPOCHS)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The correlation for batch number3 is :\n",
      "0.7290684776166821\n",
      "The correlation for batch number3 is :\n",
      "0.7096006334463993\n",
      "The correlation for batch number3 is :\n",
      "0.7001433295226278\n",
      "The correlation for batch number3 is :\n",
      "0.6739208806995669\n",
      "The correlation for batch number3 is :\n",
      "0.7028208290554693\n",
      "The correlation for batch number3 is :\n",
      "0.7109311977826535\n",
      "The correlation for batch number3 is :\n",
      "0.7030675850692871\n",
      "The correlation for batch number3 is :\n",
      "0.6305507729832571\n",
      "The correlation for batch number3 is :\n",
      "0.6157131781406932\n",
      "The correlation for batch number3 is :\n",
      "0.6283542232713288\n",
      "The correlation for batch number3 is :\n",
      "0.5507159373583025\n",
      "The correlation for batch number3 is :\n",
      "0.5405234747681081\n",
      "The correlation for batch number3 is :\n",
      "0.5304123385033863\n",
      "The correlation for batch number3 is :\n",
      "0.5376511797971465\n",
      "The correlation for batch number3 is :\n",
      "0.5405870374715676\n",
      "The correlation for batch number3 is :\n",
      "0.5477203393546923\n",
      "The correlation for batch number3 is :\n",
      "0.5585409272663694\n",
      "The correlation for batch number3 is :\n",
      "0.5855990190988338\n",
      "The correlation for batch number3 is :\n",
      "0.5992834770516994\n",
      "The correlation for batch number3 is :\n",
      "0.6162301745208945\n",
      "The correlation for batch number3 is :\n",
      "0.6090178533555483\n",
      "The correlation for batch number3 is :\n",
      "0.6075319296828487\n",
      "The correlation for batch number3 is :\n",
      "0.6052635292233878\n",
      "The correlation for batch number3 is :\n",
      "0.608483110349517\n",
      "The correlation for batch number3 is :\n",
      "0.6179988515476483\n",
      "The correlation for batch number3 is :\n",
      "0.621526460452942\n",
      "The correlation for batch number3 is :\n",
      "0.6286823558084068\n",
      "The correlation for batch number3 is :\n",
      "0.623503332449739\n",
      "The correlation for batch number3 is :\n",
      "0.6305612260816967\n",
      "The correlation for batch number3 is :\n",
      "0.6306877740966302\n",
      "The correlation for batch number3 is :\n",
      "0.6315007535467184\n",
      "The correlation for batch number3 is :\n",
      "0.6271472585105415\n",
      "The correlation for batch number3 is :\n",
      "0.633326468699723\n",
      "The correlation for batch number3 is :\n",
      "0.6415144913796463\n",
      "The correlation for batch number3 is :\n",
      "0.6419861789697742\n",
      "The correlation for batch number3 is :\n",
      "0.6436315314477349\n",
      "The correlation for batch number3 is :\n",
      "0.641690207776214\n",
      "The correlation for batch number3 is :\n",
      "0.6457234642792568\n",
      "The correlation for batch number3 is :\n",
      "0.6497053204843288\n",
      "The correlation for batch number3 is :\n",
      "0.6467132772716654\n",
      "The correlation for batch number3 is :\n",
      "0.6378634009082096\n",
      "The correlation for batch number3 is :\n",
      "0.633984447605228\n",
      "The correlation for batch number3 is :\n",
      "0.6279124602149941\n",
      "The correlation for batch number3 is :\n",
      "0.6310218240481543\n",
      "The correlation for batch number3 is :\n",
      "0.6310225746937358\n",
      "The correlation for batch number3 is :\n",
      "0.6329637607200158\n",
      "The correlation for batch number3 is :\n",
      "0.6358721846259445\n",
      "The correlation for batch number3 is :\n",
      "0.6352447465498823\n",
      "The correlation for batch number3 is :\n",
      "0.6354954021792616\n",
      "The correlation for batch number3 is :\n",
      "0.63529392313593\n",
      "The correlation for batch number3 is :\n",
      "0.6394245585049175\n",
      "The correlation for batch number3 is :\n",
      "0.6429541038503302\n",
      "The correlation for batch number3 is :\n",
      "0.6439419622134125\n",
      "The correlation for batch number3 is :\n",
      "0.6379291886733232\n",
      "The correlation for batch number3 is :\n",
      "0.6425584719780397\n",
      "The correlation for batch number3 is :\n",
      "0.6447960528912544\n",
      "The correlation for batch number3 is :\n",
      "0.6444749388012209\n",
      "The correlation for batch number3 is :\n",
      "0.63924060359413\n",
      "The correlation for batch number3 is :\n",
      "0.6267839770446997\n",
      "The correlation for batch number3 is :\n",
      "0.6304968614329732\n",
      "The correlation for batch number3 is :\n",
      "0.6337551596139398\n",
      "The correlation for batch number3 is :\n",
      "0.6240938140672232\n",
      "The correlation for batch number3 is :\n",
      "0.6239213603612129\n",
      "The correlation for batch number3 is :\n",
      "0.6258158339269323\n",
      "The correlation for batch number3 is :\n",
      "0.623403645474443\n",
      "The correlation for batch number3 is :\n",
      "0.625698741853637\n",
      "The correlation for batch number3 is :\n",
      "0.6252432516517992\n",
      "The correlation for batch number3 is :\n",
      "0.6190512889378514\n",
      "The correlation for batch number3 is :\n",
      "0.6198656216036603\n",
      "The correlation for batch number3 is :\n",
      "0.6192113245783666\n",
      "The correlation for batch number3 is :\n",
      "0.6236988337428023\n",
      "The correlation for batch number3 is :\n",
      "0.6252960350679057\n",
      "The correlation for batch number3 is :\n",
      "0.6253465064717505\n",
      "The correlation for batch number3 is :\n",
      "0.6236114148079149\n",
      "The correlation for batch number3 is :\n",
      "0.6274374539652314\n",
      "The correlation for batch number3 is :\n",
      "0.628335300080095\n",
      "The correlation for batch number3 is :\n",
      "0.628923265154917\n",
      "The correlation for batch number3 is :\n",
      "0.6299256608106387\n",
      "The correlation for batch number3 is :\n",
      "0.6313021086078964\n",
      "The correlation for batch number3 is :\n",
      "0.6334894344250918\n",
      "The correlation for batch number3 is :\n",
      "0.6373799431400359\n",
      "The correlation for batch number3 is :\n",
      "0.6397850592114331\n",
      "The correlation for batch number3 is :\n",
      "0.6352067907427199\n",
      "The correlation for batch number3 is :\n",
      "0.6328390183148118\n",
      "The correlation for batch number3 is :\n",
      "0.6251119849689736\n",
      "The correlation for batch number3 is :\n",
      "0.6228743806413706\n",
      "The correlation for batch number3 is :\n",
      "0.6204379213775515\n",
      "The correlation for batch number3 is :\n",
      "0.6182021694653305\n",
      "The correlation for batch number3 is :\n",
      "0.6176658425914144\n",
      "The correlation for batch number3 is :\n",
      "0.6134121825745088\n",
      "The correlation for batch number3 is :\n",
      "0.6131419574275504\n",
      "The correlation for batch number3 is :\n",
      "0.6107485783639408\n",
      "The correlation for batch number3 is :\n",
      "0.6143864691039831\n",
      "The correlation for batch number3 is :\n",
      "0.6094562298173573\n",
      "The correlation for batch number3 is :\n",
      "0.6120641326308053\n",
      "The correlation for batch number3 is :\n",
      "0.6160392852136125\n",
      "The correlation for batch number3 is :\n",
      "0.6166943676897798\n",
      "The correlation for batch number3 is :\n",
      "0.6162500371209055\n",
      "The correlation for batch number3 is :\n",
      "0.6158855468255887\n",
      "The correlation for batch number3 is :\n",
      "0.6205522211754539\n",
      "The correlation for batch number3 is :\n",
      "0.6183865626908319\n",
      "The correlation for batch number3 is :\n",
      "0.6205469089409895\n",
      "The correlation for batch number3 is :\n",
      "0.6225693002447341\n",
      "The correlation for batch number3 is :\n",
      "0.6253166720075295\n",
      "The correlation for batch number3 is :\n",
      "0.6207232473121782\n",
      "The correlation for batch number3 is :\n",
      "0.6194521047019437\n",
      "The correlation for batch number3 is :\n",
      "0.6232782374774589\n",
      "The correlation for batch number3 is :\n",
      "0.6236654596279194\n",
      "The correlation for batch number3 is :\n",
      "0.6208871160654819\n",
      "The correlation for batch number3 is :\n",
      "0.6212258243772779\n",
      "The correlation for batch number3 is :\n",
      "0.6228478321044274\n",
      "The correlation for batch number3 is :\n",
      "0.6237421130371626\n",
      "The correlation for batch number3 is :\n",
      "0.620199576294872\n",
      "The correlation for batch number3 is :\n",
      "0.6220580707852043\n",
      "The correlation for batch number3 is :\n",
      "0.6200755002796668\n",
      "The correlation for batch number3 is :\n",
      "0.6215918455123667\n",
      "The correlation for batch number3 is :\n",
      "0.6211283883614322\n",
      "The correlation for batch number3 is :\n",
      "0.6229953578292653\n",
      "The correlation for batch number3 is :\n",
      "0.6253738548857044\n",
      "The correlation for batch number3 is :\n",
      "0.6217961414304695\n",
      "The correlation for batch number3 is :\n",
      "0.6229658090002779\n",
      "The correlation for batch number3 is :\n",
      "0.6177711532799484\n",
      "0.6177711532799484\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#perfom compute_accuracy on validation set\n",
    "print(eval_pearson_corr(model, val_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "thesis_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
