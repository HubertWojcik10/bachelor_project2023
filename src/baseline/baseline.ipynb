{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.weight', 'classifier.out_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from transformers import XLMRobertaForSequenceClassification, XLMRobertaTokenizer, XLMRobertaConfig\n",
    "import torch\n",
    "from torch import Tensor\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "tokenizer = XLMRobertaTokenizer.from_pretrained(\"xlm-roberta-base\")\n",
    "config = XLMRobertaConfig(\n",
    "    num_labels=1,\n",
    "    output_hidden_states=False,\n",
    "    output_attentions=False,\n",
    ")\n",
    "config.vocab_size = tokenizer.vocab_size\n",
    "\n",
    "model = XLMRobertaForSequenceClassification.from_pretrained(\"xlm-roberta-base\", num_labels=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XLMRobertaForSequenceClassification(\n",
       "  (roberta): XLMRobertaModel(\n",
       "    (embeddings): XLMRobertaEmbeddings(\n",
       "      (word_embeddings): Embedding(250002, 768, padding_idx=1)\n",
       "      (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
       "      (token_type_embeddings): Embedding(1, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): XLMRobertaEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x XLMRobertaLayer(\n",
       "          (attention): XLMRobertaAttention(\n",
       "            (self): XLMRobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): XLMRobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): XLMRobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): XLMRobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (classifier): XLMRobertaClassificationHead(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "    (out_proj): Linear(in_features=768, out_features=1, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Unnamed: 0', 'pair_id', 'id1', 'id2', 'text1', 'text2', 'overall',\n",
      "       'lang1', 'lang2'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "train_data = pd.read_csv(\"../../data/train/train.csv\")\n",
    "test_data = pd.read_csv(\"../../data/test/final_test_pairs.csv\")\n",
    "\n",
    "print(train_data.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_and_shorten_sentence(text: str) -> Tensor:\n",
    "    \"\"\"\n",
    "    Tokenize the input text and shorten it to 256 tokens.\n",
    "\n",
    "    Args:\n",
    "        text (str): The input text.\n",
    "\n",
    "    Returns:\n",
    "        Tensor: The tokenized and shortened text tensor.\n",
    "    \"\"\"\n",
    "    tokenized_text = tokenizer(text, return_tensors=\"pt\", padding=False, truncation=False, add_special_tokens=False, max_length = None)\n",
    "\n",
    "    if tokenized_text[\"input_ids\"].shape[1] > 256:\n",
    "        shorten_ids =  tokenized_text[\"input_ids\"][:, :200].tolist()[0] + tokenized_text[\"input_ids\"][:, -54:].tolist()[0]\n",
    "    else:\n",
    "        shorten_ids = tokenized_text[\"input_ids\"].tolist()[0] + [tokenizer.pad_token_id] * (254 - tokenized_text[\"input_ids\"].shape[1])\n",
    "\n",
    "\n",
    "    return tokenizer.decode(shorten_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_texts(text1:str, text2:str):\n",
    "    tokenized_text = tokenizer(text1, text2, return_tensors=\"pt\", padding=\"max_length\", \n",
    "                               truncation=True, add_special_tokens=True, max_length=512)\n",
    "    return tokenized_text[\"input_ids\"], tokenized_text[\"attention_mask\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (746 > 512). Running this sequence through the model will result in indexing errors\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n"
     ]
    }
   ],
   "source": [
    "input_ids = []\n",
    "attention_mask = []\n",
    "\n",
    "\n",
    "for i in range(len(train_data)):\n",
    "    text1 = train_data[\"text1\"][i]\n",
    "    text2 = train_data[\"text2\"][i]\n",
    "    text1_truncated = tokenize_and_shorten_sentence(text1)\n",
    "    text2_truncated = tokenize_and_shorten_sentence(text2)\n",
    "\n",
    "    text_input_ids, text_attention_mask = tokenize_texts(text1_truncated, text2_truncated)\n",
    "    input_ids.append(text_input_ids.tolist()[0])\n",
    "    attention_mask.append(text_attention_mask.tolist()[0])\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = torch.tensor(train_data[\"overall\"]).float()\n",
    "data = TensorDataset(torch.tensor(input_ids).long(), torch.tensor(attention_mask).long(), score.view(-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = int(0.1 * len(data))\n",
    "test_size = len(data) - train_size\n",
    "train_dataset, val_dataset = torch.utils.data.random_split(data, [train_size, test_size])\n",
    "\n",
    "loader = DataLoader(train_dataset, batch_size=8, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average training loss: 10.52\n",
      "current loss: 10.515270233154297\n",
      "logits: tensor([[ 0.0944],\n",
      "        [-0.0041],\n",
      "        [ 0.0993],\n",
      "        [ 0.0851]], grad_fn=<AddmmBackward0>)\n",
      "average training loss: 6.66\n",
      "current loss: 5.921501159667969\n",
      "logits: tensor([[0.2391],\n",
      "        [0.2604],\n",
      "        [1.0051],\n",
      "        [1.1593]], grad_fn=<AddmmBackward0>)\n",
      "average training loss: 5.75\n",
      "current loss: 4.26238489151001\n",
      "logits: tensor([[0.4642],\n",
      "        [0.9886],\n",
      "        [2.1218],\n",
      "        [0.5776]], grad_fn=<AddmmBackward0>)\n",
      "average training loss: 4.72\n",
      "current loss: 1.5303306579589844\n",
      "logits: tensor([[1.3579],\n",
      "        [1.7493],\n",
      "        [0.5731],\n",
      "        [1.2010]], grad_fn=<AddmmBackward0>)\n",
      "average training loss: 4.11\n",
      "current loss: 0.7820334434509277\n",
      "logits: tensor([[3.1095],\n",
      "        [3.4597],\n",
      "        [3.0207],\n",
      "        [3.3843]], grad_fn=<AddmmBackward0>)\n",
      "average training loss: 3.62\n",
      "current loss: 1.4791193008422852\n",
      "logits: tensor([[2.0832],\n",
      "        [3.1919],\n",
      "        [1.6894],\n",
      "        [2.0780]], grad_fn=<AddmmBackward0>)\n",
      "average training loss: 3.33\n",
      "current loss: 1.838634729385376\n",
      "logits: tensor([[3.3209],\n",
      "        [3.3139],\n",
      "        [1.4680],\n",
      "        [1.1084]], grad_fn=<AddmmBackward0>)\n",
      "average training loss: 3.12\n",
      "current loss: 2.000547170639038\n",
      "logits: tensor([[2.4985],\n",
      "        [3.5529],\n",
      "        [3.4246],\n",
      "        [1.6125]], grad_fn=<AddmmBackward0>)\n",
      "average training loss: 2.96\n",
      "current loss: 0.6336577534675598\n",
      "logits: tensor([[2.6075],\n",
      "        [1.6561],\n",
      "        [3.0318],\n",
      "        [3.5987]], grad_fn=<AddmmBackward0>)\n",
      "average training loss: 2.80\n",
      "current loss: 1.9221892356872559\n",
      "logits: tensor([[2.6694],\n",
      "        [2.5842],\n",
      "        [2.7073],\n",
      "        [3.0006]], grad_fn=<AddmmBackward0>)\n",
      "average training loss: 2.72\n",
      "current loss: 2.3244824409484863\n",
      "logits: tensor([[3.2265],\n",
      "        [3.1946],\n",
      "        [3.0010],\n",
      "        [3.1545]], grad_fn=<AddmmBackward0>)\n",
      "average training loss: 2.58\n",
      "current loss: 1.2002029418945312\n",
      "logits: tensor([[3.3286],\n",
      "        [2.8776],\n",
      "        [2.4328],\n",
      "        [1.9000]], grad_fn=<AddmmBackward0>)\n",
      "average training loss: 2.48\n",
      "current loss: 2.349787712097168\n",
      "logits: tensor([[3.1543],\n",
      "        [3.1471],\n",
      "        [2.0130],\n",
      "        [2.7128]], grad_fn=<AddmmBackward0>)\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'loss_values' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[106], line 28\u001b[0m\n\u001b[1;32m     25\u001b[0m       \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlogits: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlogits\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     27\u001b[0m \u001b[38;5;66;03m# Store the loss value for plotting the learning curve.\u001b[39;00m\n\u001b[0;32m---> 28\u001b[0m \u001b[43mloss_values\u001b[49m\u001b[38;5;241m.\u001b[39mappend(total_loss)\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maverage training loss: \u001b[39m\u001b[38;5;132;01m{0:.2f}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(total_loss))\n",
      "\u001b[0;31mNameError\u001b[0m: name 'loss_values' is not defined"
     ]
    }
   ],
   "source": [
    "EPOCHS = 1\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-5)\n",
    "total_loss = 0\n",
    "model.train()\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "   for idx, (ids, att, val) in enumerate(loader):\n",
    "      ids, att, val = ids.to(device), att.to(device), val.to(device)\n",
    "\n",
    "      outputs = model(input_ids=ids, attention_mask=att, labels=val)\n",
    "      loss, logits = outputs[:2]\n",
    "\n",
    "      optimizer.zero_grad()\n",
    "      loss.backward()\n",
    "      optimizer.step()\n",
    "      #scheduler.step()\n",
    "      total_loss += loss.item()\n",
    "\n",
    "      torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "\n",
    "      if idx % 10 == 0:\n",
    "         print(\"average training loss: {0:.2f}\".format(total_loss / (idx+1)))\n",
    "         print(\"current loss:\", loss.item())\n",
    "\n",
    "         print(f\"logits: {logits}\")\n",
    "\n",
    "   # Store the loss value for plotting the learning curve.\n",
    "   loss_values.append(total_loss)\n",
    "   print(\"average training loss: {0:.2f}\".format(total_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions: tensor([[[-0.0474],\n",
      "         [ 0.2472],\n",
      "         [ 0.2336],\n",
      "         [ 0.2687],\n",
      "         [ 0.3569],\n",
      "         [ 0.2603],\n",
      "         [ 0.3908],\n",
      "         [-0.0327]]], grad_fn=<ViewBackward0>)\n"
     ]
    }
   ],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "thesis_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
